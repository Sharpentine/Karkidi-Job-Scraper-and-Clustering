# Job Data Analysis and Clustering

This project scrapes job data from the Karkidi website, processes the "Key Skills" information, and applies KMeans clustering to group jobs based on their skill requirements.

## Features

- **Job Data Scraping:** Fetches job postings based on a search term and number of pages from the Karkidi website.
- **Skill Extraction:** Extracts key skills and other relevant job details from each posting.
- **Data Cleaning:** Processes the extracted skill text for clustering.
- **Clustering:** Applies KMeans clustering on the vectorized skill data.
- **Elbow Method:** Includes a function to generate an Elbow plot to help determine the optimal number of clusters.
- **Model Persistence:** Saves the trained clustering model and vectorizer for future use.
- **Results Output:** Saves the scraped and clustered job data to a CSV file.

## Project Structure

- `your_notebook_name.ipynb`: The main Google Colab notebook containing the code for scraping, clustering, and execution.
- `elbow_plot.png`: An image generated by the Elbow Method function, visualizing the optimal number of clusters.
- `job_model.pkl`: A file containing the saved TF-IDF vectorizer and KMeans model.
- `clustered_jobs.csv`: A CSV file containing the scraped job data with an added "Cluster" column.

## Setup and Usage

1.  **Open the Notebook:** Open the provided Google Colab notebook (`your_notebook_name.ipynb`).
2.  **Install Dependencies:** Ensure you have the necessary libraries installed. In Colab, you might need to install `joblib` if it's not already available. The other libraries (`requests`, `bs4`, `pandas`, `sklearn`, `matplotlib`) are typically pre-installed. If not, you can install them using pip: bash !pip install requests beautifulsoup4 pandas scikit-learn matplotlib joblib
3.  **Run the Cells:** Execute the code cells in the notebook sequentially.
4.  **Select Number of Clusters:** When prompted by the Elbow Method visualization, input the desired number of clusters based on the generated plot.
5.  **View Results:** After execution, two files will be generated in your Colab environment's file browser:
    - `clustered_jobs.csv`: Contains the scraped and clustered job data.
    - `job_model.pkl`: Contains the saved model artifacts.
6.  **Download Files (Optional):** You can download these files from the Colab file browser.

## Code Details

- The `fetch_jobs_from_karkidi` function handles the web scraping using `requests` and `BeautifulSoup`.
- The `clean_skills_column` function preprocesses the skill data.
- The `plot_elbow_chart` function generates the Elbow plot for cluster selection.
- The `perform_clustering` function orchestrates the vectorization and KMeans clustering.
- The `persist_model` and `retrieve_model` functions handle saving and loading the model artifacts using `joblib`.
- The `if __name__ == "__main__":` block demonstrates how to run the complete pipeline.

## Dependencies

- `Python`
- `requests`
- `beautifulsoup4`
- `pandas`
- `scikit-learn`
- `matplotlib`
- `joblib`

## Notes

- The scraping function includes a delay (`time.sleep(1)`) to avoid overwhelming the website. You might need to adjust this based on the website's terms of service or your usage patterns.
- The Elbow Method provides a visual guide for choosing the number of clusters, but the optimal number can sometimes be subjective.
    
